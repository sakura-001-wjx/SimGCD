## 一、消融实验     

​	 原论文中关于消融实验采取如下步骤：+SL（将UNO的自标记分类目标用于GCD），+BR（在主干网络的特征输出上构建参数分类器），+SD（加入自蒸馏），+TW（加入教师预热），+JT（将表示学习和分类目标损失结合）；之前汇报时候我提到了步骤的排列组合，但是我发现并非简单的排列组合，而是必须采取一定的顺序，比如在+SD后才可以+TW（引入教师温度才可以对其预热）、+SL后才可以+JT（引入分类目标损失才可以把他和表示学习耦合）等，因此我采取的消融实验步骤如下：+SL，+SD，+BR，+TW，+JT。（readme中提到的精度具体可见日志）

​	（注：压缩包中的train.py和model.py为原论文的代码，train1-5.py为我实现的每步消融实验对应的train.py，model.py同理，但是model1.py使用model，不做改进）

### 1.1 +SL+SD+BR+TW+JT（不进行消融）

​	我对源代码加了简单的修改（去除了使用多个数据集的部分），运行了两次结果，得到的最终all准确率（下文准确率均指此准确率）分别为0.6032和0.6130。与论文中的较为接近（但是论文作者说他的是0.615+-0.005）

### 1.2 +SL+SD+BR+TW

​	我主要把原来代码中的联合损失修改为了表示学习损失和分类目标损失，主要修改如下：

   修改前(提到的修改均为主要修改部分)：
   ![Image text](/data/pictures/picture2.png)

   修改后：
   ![Image text](/data/pictures/picture1.png)

​	我一共运行了两次代码，一次的准确率为0.5939，一次为0.6174，略有波动，但是有一次居然取得了更好的效果，可能是参数设置的原因（我未对参数做改动，但是1.1中我参数是直接使用作者.sh文件中的参数，应该较为稳定）。

### 1.3 +SL+SD+BR

​    在本步中我去除了教师温度预热，但是恒定的温度仍然对实验结果影响巨大，我做了多次实验，尝试的温度包括0.07，0.04，0.035，0.03，0.02，0.01，但是其中有一些与论文中提到的相差很大，甚至只有0.3，0.4左右的精度，而温度为0.07时，精度先上升到了0.5，后来居然又下降到了0.3，由于跑一轮（200epochs）需要2个小时，我很多只跑了100epochs左右，我发现当我选取温度为0.035的时候，精度在100轮左右后仍然有缓慢上升，最终我只选择0.035跑完了200epochs，精度为0.5151，与论文中的仍然有所差距。（我推测本步消融中温度的选取对后面的消融实验仍然有至关重要的影响）（Old和New的精度也有点奇怪）

   修改前：
   ![Image text](/data/pictures/picture3.png)

   修改后：
   ![Image text](/data/pictures/picture4.png)

### 1.4 +SL+SD

​	在本步中我将主干网络提取的特征向量修改为了投影器修改的特征向量，发现精度逐步提升，最高可到达0.5左右，但是后续又缓慢下降，最后趋于稳定，精准度为0.4849，我猜测仍然是先前自蒸馏温度选取的原因。

   修改前：
   ![Image text](/data/pictures/picture5.png)

   修改后：
   ![Image text](/data/pictures/picture6.png)

### 1.5 +SL

   在本步中我去除了自蒸馏损失（删除了class DistillLoss(nn.Module)），精度进一步下降，第一次运行时，精度开始达到了0.36，之后居然下降到了0.28，还在下降，这个时候只跑了40epochs，我停止了运行，开始思考代码，我想是不是拆开损失后两次backward之间没有清除梯度导致的，我又做了点修改，结果跑了40epochs感觉还是不对劲，赶紧停止了，又换回了修改前的做法。这一次我让程序完整运行完了200eopochs，最终的精度为0.3225。

### 1.6 GCD

   本部分我去除了UNO的自标记损失，但是精度效果仍然与论文中相比有很大差距，最终的精度为0.3432,相对第五步略有提升，和论文中说的一样，因为是自标记的影响，会稍微降低精度。

   修改前：
   ![Image text](/data/pictures/picture7.png)

   修改后：
   ![Image text](/data/pictures/picture8.png)

## 二、实验结果图对比

   论文结果：
   ![Image text](/data/pictures/picture9.png)

   我的结果：（1.1和1.2分别选取0.6130和0.5939）
   ![Image text](/data/pictures/picture10.png)
## 三、我的反思
​	起初在简单复现原论文的时候，跑出来的精度与论文基本相同，但是和作者在readme.md写到的还是有1%左右的浮动，已经超过了作者说的浮动范围，但是我并未对代码做什么修改，只是简单修改了一下路径，文件名等；第一步消融实验时，开始跑的结果居然相比之前还提升了，第二次跑的时候又有所下降，我猜测可能是我参数没有选好，后面的大部分，我均未对作者的.sh文件参数做修改，只是简单修改了预热的温度值；后面几步消融实验类似，仍然出现各种各样奇怪的现象，有什么精度居然先升后面又降低很多的（可能是学习率大的原因？可是已经0.l了，加上那个余弦变化已经很小了），还有中间平稳了后面又缓慢升起来的等等。
​	总之，我反思后续几步消融实验效果不好的原因是作者在每一步都采取了不同的参数，但是他只提供了最后完整版的参数，把这个写到了readme文件里。而且我跑一轮代码需要2个小时左右，要不断试参数的话时间可能不够，而且有时候我看情况不对，可能只跑了几十轮就结束了，没有跑完整，这个参数太多了，我调整的也只是那个教师温度的参数。
​	此外我还有一个问题，关于最后一步消融实验，作者说把UNO的自标注分类目标加到优化中，可以把原来的非参数分类器GCD转变为参数分类器，我的想法是，因为作者在model文件中引入了DINOHEAD，里面有多层感知机，这个应该是作者指的参数分类器，需要估计这个东西的参数；但是我看论文说原来的GCD使用了VIT，我想这不也是神经网络吗，应该里面也有很多参数，为什么说GCD是非参数分类器，这个我不太理解。
​	最后我还是觉得我的代码能力还需要加强，理论知识的基础也太薄弱了，一个代码报错就需要调整很差时间。这一次的代码测试，我确定可以感觉到，在各个方面增强了我的水平，我相信在不断的学习中我会取得更多的进步。
